experiment_setup:
  executable: 'code_transformer/experiments/great/code_summarization.py'

data_setup:
  language: 'python,javascript,ruby,go'
  use_validation: True
  num_sub_tokens: 5
  num_subtokens_output: 6
  use_pointer_network: True

data_transforms:
  max_distance_mask: None
  relative_distances:
    - ppr
    - ancestor_sp
    - sibling_sp
    - shortest_paths

  distance_binning:
    type: 'exponential'
    growth_factor: 1.3
    n_fixed_bins: 9

model:
  with_cuda: False
  label_smoothing: 0.1
  lm_encoder:
    transformer_config:
      embed_dim: 16
      num_layers: 3
      num_heads: 8
      ff_dim: 16
      dropout_rate: 0.2
  lm_decoder:
    output_nonlinearity: None
    n_layers: 1
    decoder_dropout: 0
    decoder_nhead: 8
    decoder_dim_feedforward: 15
    decoder_activation: 'gelu'
    use_teacher_forcing: True
    pointer_attention_type: 'additive'
    use_pointer_query_linear: False
    use_pointer_query_self_attention: False
    attend_cls_token: False

optimizer:
  optimizer: 'Adam'
  learning_rate: 8e-5
  reg_scale: 3e-5

  #scheduler: 'OneCycleLR'
  #scheduler_params:
  #  max_lr: 1e-4
  #  steps_per_epoch: 4000
  #  epochs: 30
  #  pct_start: 0.3

  #scheduler: 'MultiStepLR'
  #scheduler_params:
  #  milestones: [1500, 5000]
  #  gamma: 0.1

training:
  random_seed: 456
  batch_size: 8
  simulated_batch_size: 128
  simulated_batch_size_valid: 1280
  accumulate_tokens_batch: False
  validate_every: 100
  persistent_snapshot_every: 100
  early_stopping_patience: 20
  max_validation_samples: 50000
  metrics:
    - top1_accuracy
    - top5_accuracy
    - non_trivial_accuracy
    - precision
    - recall
    - f1_score
    - micro_f1_score
    - rouge-2
    - rouge-l

