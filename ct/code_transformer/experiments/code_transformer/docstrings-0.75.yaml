experiment_setup:
  executable: 'code_transformer/experiments/code_transformer/code_summarization.py'

data_setup:
  language: python
  docstrings: True
  use_validation: True
  num_sub_tokens: 5                     # Number of sub-tokens that input tokens should be split into
  num_subtokens_output: 16               # Number of sub-tokens of the method name to predict
  use_only_ast: False                   # Whether to use the only-ast ablation
  mask_all_tokens: False                # Only relevant if use_only_ast=True. Replaces all input tokens with a dummy token
  use_no_punctuation: True              # Whether to drop punctuation tokens before feeding the snippets into the model
  use_pointer_network: True             # Whether to use a pointer network in the decoder
  sort_by_length: False                 # Whether to sort loaded slices by number of tokens. Useful to minimize amount of zero-padding needed
  shuffle: False                        # Whether load order of snippets should be randomized
  chunk_size: 32                        # Only relevant if shuffle=True and sort_by_lenght=True. Snippets will be chunked into chunks of `chunk_size`, which will then be randomly shuffled.

data_transforms:
  max_distance_mask: None

  relative_distances:                   # Which relative distances to use (have to be pre-computed in stage 2 preprocessing)
    - ppr
    - ancestor_sp
    - sibling_sp
    - shortest_paths

  distance_binning:                     # Distance binning for dealing with real-valued distances
    type: 'exponential'                 # "exponential" or "equal". Exponential binning has more diversified (smaller) bins for smaller distances
    growth_factor: 1.3
    n_fixed_bins: 9

model:
  with_cuda: True                       # Run model on GPU
  label_smoothing: 0.1                  # Apply label smoothing to ground truth
  lm_encoder:                           # Hyperparameters of the encoder
    input_nonlinearity: 'tanh'
    num_languages: None                   # only relevant for multi-language datasets. How many different languages have been fused together
    transformer:                          # CodeTransformer hyperparameters
      num_layers: 1
      encoder_layer:
        d_model: 768 # 1024                       # Internal embedding dimension
        nhead: 6 # 8                            # Number of attention heads
        dim_feedforward: 1536 # 2048               # Dimension of feed-forward layer
        dropout: 0.2
        activation: 'gelu'
        use_content_content: True           # Whether to use the content-content term in attention computation
        use_content_pos: True               # Whether to use the content-content term in attention computation
        use_pos_content: True               # Whether to use the content-content term in attention computation
        use_pos_pos: True                   # Whether to use the content-content term in attention computation
        use_token_distances: True           # Whether to also compute the simple hop-distance between the input tokens
  lm_decoder:                           # Hyperparameters of the decoder
    output_nonlinearity: None
    n_layers: 1
    decoder_dropout: 0.2
    decoder_nhead: 6 #8
    decoder_dim_feedforward: 1536 #2048
    decoder_activation: 'gelu'
    use_teacher_forcing: True             # Whether to use teacher forcing during training (Label is fed into decoder instead of prediction for previous position)
    pointer_attention_type: 'additive'    # Attention type in Pointer Network. "scaled_dot_product", "multihead" or "additive"
    use_pointer_query_self_attention: False # Whether to use self-attention between pointer query and decoder input
    concat_query_and_pointer: True        # Whether to also use the query-stream of the encoder output to guide the pointer query
    attend_cls_token: False               # Whether to mask the CLS token for attention

optimizer:
  scheduler: 'MyCyclicLR'
  optimizer: 'Adam'
  simulated_batch_size: 512
  learning_rate: 1e-3
  reg_scale: 0

training:
  start_from_snapshot_run_id: None
  start_from_snapshot_iteration: None
  project_name: ct-pythonxglue
  random_seed: 456
  batch_size: 8
  simulated_batch_size: 512         # Gradient Accumulation. After how many samples the model parameters should be updated
  simulated_batch_size_valid: 1280  # Over how many samples validation metrics should be calculated
  accumulate_tokens_batch: False
  device: 'cuda:0'
  validate_every: 200               # Counted in number of parameter updates (simulated_batch_size). How often approximate evaluation should be done
  epochs: 5
  max_validation_samples: 10000
  metrics:
    - f1_score
    - rouge_2
